{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset from: https://zindi.africa/competitions/digital-africa-plantation-counting-challenge","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_pytorch\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom PIL import Image\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nfrom efficientnet_pytorch import EfficientNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-12T18:42:24.978418Z","iopub.execute_input":"2023-03-12T18:42:24.978749Z","iopub.status.idle":"2023-03-12T18:42:43.065052Z","shell.execute_reply.started":"2023-03-12T18:42:24.978719Z","shell.execute_reply":"2023-03-12T18:42:43.063982Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (4.4.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=59dbedd46746d50bebde5ab9b716188138240d23eb7e266afef89580169b5cc9\n  Stored in directory: /root/.cache/pip/wheels/96/3f/5f/13976445f67f3b4e77b054e65f7f4c39016e92e8358fe088db\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"Train = pd.read_csv('/kaggle/input/digital-africa-plantation-counting-challenge/Train.csv')\nTest = pd.read_csv('/kaggle/input/digital-africa-plantation-counting-challenge/Test.csv')\nSampleSubmission = pd.read_csv('/kaggle/input/digital-africa-plantation-counting-challenge/SampleSubmission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.067132Z","iopub.execute_input":"2023-03-12T18:42:43.067672Z","iopub.status.idle":"2023-03-12T18:42:43.099657Z","shell.execute_reply.started":"2023-03-12T18:42:43.067638Z","shell.execute_reply":"2023-03-12T18:42:43.098753Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"int(Train.shape[0]*0.85), int(Train.shape[0] - Train.shape[0]*0.85)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.102792Z","iopub.execute_input":"2023-03-12T18:42:43.103062Z","iopub.status.idle":"2023-03-12T18:42:43.114781Z","shell.execute_reply.started":"2023-03-12T18:42:43.103037Z","shell.execute_reply":"2023-03-12T18:42:43.113823Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(1701, 300)"},"metadata":{}}]},{"cell_type":"code","source":"Train","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.117481Z","iopub.execute_input":"2023-03-12T18:42:43.118131Z","iopub.status.idle":"2023-03-12T18:42:43.135539Z","shell.execute_reply.started":"2023-03-12T18:42:43.118097Z","shell.execute_reply":"2023-03-12T18:42:43.134551Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                ImageId  Target\n0     Id_jdqw9hlv6j.png    14.0\n1     Id_6xtrolmuvc.png    18.0\n2     Id_2m49sj3xd9.png     0.0\n3     Id_9jwg5pcnn4.png    28.0\n4     Id_vnm6e8n0p3.png    21.0\n...                 ...     ...\n1997  Id_n2vxw7x9c5.png    14.0\n1998  Id_1cx78gejxc.png     0.0\n1999  Id_kbgnlekbjm.png    13.0\n2000  Id_uzb88simbg.png     0.0\n2001  Id_qi2hbnooa9.png     0.0\n\n[2002 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Id_jdqw9hlv6j.png</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Id_6xtrolmuvc.png</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Id_2m49sj3xd9.png</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Id_9jwg5pcnn4.png</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Id_vnm6e8n0p3.png</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>Id_n2vxw7x9c5.png</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>Id_1cx78gejxc.png</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>Id_kbgnlekbjm.png</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>2000</th>\n      <td>Id_uzb88simbg.png</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>Id_qi2hbnooa9.png</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2002 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def otsu_threshold(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresh\ndef refine_image(image):\n    # apply Otsu's thresholding\n    thresh = otsu_threshold(image)\n    # define structuring element for erosion and dilation\n    kernel = np.ones((3, 3), np.uint8)\n    # perform erosion to remove small white regions or thin white lines\n    eroded = cv2.erode(thresh, kernel, iterations=1)\n    # perform dilation to remove small black regions or thin black lines\n    dilated = cv2.dilate(eroded, kernel, iterations=1)\n\n    return dilated","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.136786Z","iopub.execute_input":"2023-03-12T18:42:43.137552Z","iopub.status.idle":"2023-03-12T18:42:43.144993Z","shell.execute_reply.started":"2023-03-12T18:42:43.137517Z","shell.execute_reply":"2023-03-12T18:42:43.144021Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class d(Dataset):\n    def __init__(self, csv_file, root_images, is_train=True, is_inference=False, transform=None ):\n        \n        self.root_images = root_images\n        self.transform   = transform\n        self.is_inference= is_inference\n        \n        if is_inference:\n            self.csv_file = csv_file\n        else:\n            if is_train:\n                self.csv_file = csv_file[:int(csv_file.shape[0]*0.85)].reset_index(drop=True)\n            else:\n                self.csv_file = csv_file[int(csv_file.shape[0]*0.85):].reset_index(drop=True)        \n        \n    def __len__(self):\n        return self.csv_file.shape[0]\n    \n    def __getitem__(self, index):\n        \n        root_and_dir = self.csv_file['ImageId'][index]\n        if not self.is_inference:\n            label = self.csv_file['Target'][index]\n    \n        image = np.array(Image.open(os.path.join(self.root_images, root_and_dir)).convert('RGB'))\n        \n        if self.transform is not None:\n            augmentations = self.transform(image=image)\n            image         = augmentations['image']\n            \n        if not self.is_inference:\n            return image, torch.as_tensor(label)\n    \n        return image","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.146313Z","iopub.execute_input":"2023-03-12T18:42:43.146810Z","iopub.status.idle":"2023-03-12T18:42:43.158567Z","shell.execute_reply.started":"2023-03-12T18:42:43.146775Z","shell.execute_reply":"2023-03-12T18:42:43.157632Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class PalmOilDataset(Dataset):\n    def __init__(self, image_paths, labels=None, is_train=True, is_inference=False, transform=None):\n        self.root = \"/kaggle/input/digital-africa-plantation-counting-challenge/images/\"\n        self.image_paths = image_paths\n        self.labels = labels\n        self.is_inference = is_inference\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = np.array(Image.open(self.root+img_path).convert('RGB'))\n        if not self.is_inference:\n            label = self.labels[idx]\n            \n        if self.transform is not None:\n            augmentations = self.transform(image=image)\n            image         = augmentations['image']\n            \n        if not self.is_inference:\n            return image, torch.as_tensor(label)\n    \n        return image","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.159979Z","iopub.execute_input":"2023-03-12T18:42:43.160440Z","iopub.status.idle":"2023-03-12T18:42:43.170838Z","shell.execute_reply.started":"2023-03-12T18:42:43.160406Z","shell.execute_reply":"2023-03-12T18:42:43.169792Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# LR = 1e-3\n# BS = 4\n# NE = 100\nH  = 1024\nW  = 1024\n# train_file = Train\n# test_file  = Test\n# image_path = '/kaggle/input/digital-africa-plantation-counting-challenge/images'","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.172373Z","iopub.execute_input":"2023-03-12T18:42:43.172843Z","iopub.status.idle":"2023-03-12T18:42:43.182551Z","shell.execute_reply.started":"2023-03-12T18:42:43.172808Z","shell.execute_reply":"2023-03-12T18:42:43.181551Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"normalize = A.Normalize(\n    mean = [0.5,0.5,0.5],\n    std  = [0.5,0.5,0.5], max_pixel_value=255\n)\n\ntrain_transform = A.Compose([\n    A.Resize(H,W),\n    A.Blur(p=0.2),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    normalize,\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(H,W),\n    normalize,\n    ToTensorV2(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.183892Z","iopub.execute_input":"2023-03-12T18:42:43.184324Z","iopub.status.idle":"2023-03-12T18:42:43.193734Z","shell.execute_reply.started":"2023-03-12T18:42:43.184284Z","shell.execute_reply":"2023-03-12T18:42:43.192850Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# train_ds     = d(train_file, image_path, is_train=True, is_inference=False, transform=train_transform )\n# train_loader = DataLoader(train_ds, batch_size=BS, shuffle=True)\n\n# val_ds     = d(train_file, image_path, is_train=False, is_inference=False, transform=val_transform )\n# val_loader = DataLoader(val_ds, batch_size=BS, shuffle=False)\n\n# test_ds     = d(test_file, image_path, is_train=False, is_inference=True, transform=val_transform )\n# test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.197394Z","iopub.execute_input":"2023-03-12T18:42:43.197739Z","iopub.status.idle":"2023-03-12T18:42:43.203693Z","shell.execute_reply.started":"2023-03-12T18:42:43.197714Z","shell.execute_reply":"2023-03-12T18:42:43.202636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.model = EfficientNet.from_pretrained('efficientnet-b3')\n        self.fc1 = nn.Linear(1000, 512)\n        self.fc2 = nn.Linear(512, 128)\n        self.fc3 = nn.Linear(128,1)\n        \n    def forward(self, image):\n        x = self.model(image)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n#         x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.205517Z","iopub.execute_input":"2023-03-12T18:42:43.205898Z","iopub.status.idle":"2023-03-12T18:42:43.214895Z","shell.execute_reply.started":"2023-03-12T18:42:43.205871Z","shell.execute_reply":"2023-03-12T18:42:43.213972Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Model_Trainer:\n    def __init__(self, Train, Test, Model, Dataset, train_transform, val_transform):\n        self.Model = Model\n        self.Dataset = Dataset\n        self.Test = Test\n        \n        self.optimizer = optim.Adam(Model.parameters(), lr=1e-3)\n        self.loss_fn = nn.MSELoss().to('cuda')\n        self.es = 0\n        self.X = Train['ImageId']\n        self.y = Train['Target']\n        self.train_transform = train_transform\n        self.val_transform = val_transform\n        self.NE = 10\n        \n    \n    def trainer(self, train_loader, val_loader, encoder, NE, opt, loss_fn):\n        best_loss = 999999999\n        for epoch in range(NE):\n            print('------------------------------- Epoch: '+str(epoch))\n            encoder.train()\n            for x,y in tqdm(train_loader):\n                x = x.to('cuda').to(torch.float32)\n                y = y.to(torch.float).unsqueeze(1).to('cuda')\n\n                preds = encoder(x).to(torch.float)\n\n                loss = loss_fn(preds, y)\n\n                encoder.zero_grad()\n                loss.backward()\n                opt.step()\n\n            running_loss = 0\n            encoder.eval()\n            with torch.no_grad():\n                for x,y in tqdm(val_loader):\n                    x = x.to('cuda').to(torch.float32)\n                    y = y.to(torch.float).unsqueeze(1)\n\n                    preds = encoder(x).to(torch.float)\n                    running_loss += np.sqrt(mean_squared_error(preds.cpu(), y))\n            print(f'Loss function: {running_loss/len(val_loader)}')\n            new_loss = running_loss/len(val_loader)\n            if new_loss < best_loss:\n                best_loss = new_loss\n                es   = 0\n                filepath = 'encoder-%d.pkl' % epoch\n                print(\"Saving CheckPoint\")\n                torch.save(encoder.state_dict(), filepath)\n\n            else:\n                es +=1\n\n            if es == 3 :\n                break\n        return filepath, best_loss\n    \n    def inference(self, loader, filepath):\n        inf = Net().to('cuda')\n        inf.load_state_dict(torch.load(filepath))\n        all_preds = np.array([])\n        with torch.no_grad():\n            for x in tqdm(loader):\n                x = x.to('cuda').to(torch.float32)\n                preds = inf(x).to(torch.float)\n            \n                all_preds = np.append(all_preds, preds.cpu())\n            \n        print('Done!')\n        return all_preds                            \n        \n    def KFold_training(self):\n        test_pred = []\n        fold = KFold(n_splits=5, shuffle=True)\n        i = 0\n        for train_index, test_index in fold.split(self.X, self.y):\n            X_train, X_test = self.X.iloc[train_index].reset_index(drop=True), self.X.iloc[test_index].reset_index(drop=True)\n            y_train, y_test = self.y.iloc[train_index].reset_index(drop=True), self.y.iloc[test_index].reset_index(drop=True)\n            train_ds = self.Dataset(X_train, y_train, is_train=True, is_inference=False, transform=self.train_transform)\n            val_ds = self.Dataset(X_test, y_test, is_train=False, is_inference=False, transform=self.val_transform)\n            train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n            val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n            \n            filepath, best_loss = self.trainer(train_loader, val_loader,self.Model,self.NE,self.optimizer,self.loss_fn)\n            print(f\"Fold {i}, Best Loss:{best_loss}\")\n            i += 1\n            test_ds     = self.Dataset(self.Test['ImageId'], is_train=False, is_inference=True, transform=self.val_transform )\n            test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n            preds = self.inference(test_loader, filepath)\n            test_pred.append(preds)\n        return test_pred","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.216457Z","iopub.execute_input":"2023-03-12T18:42:43.217047Z","iopub.status.idle":"2023-03-12T18:42:43.238553Z","shell.execute_reply.started":"2023-03-12T18:42:43.217008Z","shell.execute_reply":"2023-03-12T18:42:43.237583Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"encoder = Net().to('cuda')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:43.241780Z","iopub.execute_input":"2023-03-12T18:42:43.242222Z","iopub.status.idle":"2023-03-12T18:42:47.912511Z","shell.execute_reply.started":"2023-03-12T18:42:43.242176Z","shell.execute_reply":"2023-03-12T18:42:47.911450Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/47.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b172fa04cb7e4ce8aad6953b68186e88"}},"metadata":{}},{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b3\n","output_type":"stream"}]},{"cell_type":"code","source":"model_trainer = Model_Trainer(Train, Test, encoder, PalmOilDataset, train_transform, val_transform)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:42:47.913879Z","iopub.execute_input":"2023-03-12T18:42:47.914884Z","iopub.status.idle":"2023-03-12T18:42:47.925590Z","shell.execute_reply.started":"2023-03-12T18:42:47.914844Z","shell.execute_reply":"2023-03-12T18:42:47.924390Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"predictions = model_trainer.KFold_training()","metadata":{"execution":{"iopub.status.busy":"2023-03-12T18:43:01.354631Z","iopub.execute_input":"2023-03-12T18:43:01.355372Z","iopub.status.idle":"2023-03-12T21:30:42.800129Z","shell.execute_reply.started":"2023-03-12T18:43:01.355336Z","shell.execute_reply":"2023-03-12T21:30:42.799041Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [06:22<00:00,  1.05it/s]\n100%|██████████| 101/101 [00:44<00:00,  2.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 4.076897943078881\nSaving CheckPoint\n------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:36<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:37<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.4727023202593963\nSaving CheckPoint\n------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:36<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:35<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 11.144608857608077\n------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:36<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:37<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 5.813493684966965\n------------------------------- Epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:36<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:35<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 4.537652416394488\nFold 0, Best Loss:2.4727023202593963\nLoaded pretrained weights for efficientnet-b3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 215/215 [01:36<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done!\n------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:35<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:34<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 3.5524765642931557\nSaving CheckPoint\n------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:36<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:35<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.7482660233900686\nSaving CheckPoint\n------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:35<00:00,  1.19it/s]\n100%|██████████| 101/101 [00:34<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.0730531773354746\nSaving CheckPoint\n------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:35<00:00,  1.20it/s]\n100%|██████████| 101/101 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.304100195812707\n------------------------------- Epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:35<00:00,  1.20it/s]\n100%|██████████| 101/101 [00:34<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.124254792338551\n------------------------------- Epoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:34<00:00,  1.20it/s]\n100%|██████████| 101/101 [00:35<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.405992989787961\nFold 1, Best Loss:2.0730531773354746\nLoaded pretrained weights for efficientnet-b3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 215/215 [01:19<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done!\n------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:32<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.1640609958022834\nSaving CheckPoint\n------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:32<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:35<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.18957539383322\n------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.039962021112442\nSaving CheckPoint\n------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:30<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 1.9959210926294326\nSaving CheckPoint\n------------------------------- Epoch: 4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.3133952951431276\n------------------------------- Epoch: 5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:30<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.052705709169386\n------------------------------- Epoch: 6\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.1510331520438193\nFold 2, Best Loss:1.9959210926294326\nLoaded pretrained weights for efficientnet-b3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 215/215 [01:21<00:00,  2.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done!\n------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.10954861164093\nSaving CheckPoint\n------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:32<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:35<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.483038855493069\n------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.6560401207208635\n------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 3.1485883367061613\nFold 3, Best Loss:2.10954861164093\nLoaded pretrained weights for efficientnet-b3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 215/215 [01:21<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done!\n------------------------------- Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 1.452349308207631\nSaving CheckPoint\n------------------------------- Epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:32<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 3.0268958085775375\n------------------------------- Epoch: 2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.174665236771107\n------------------------------- Epoch: 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 401/401 [05:31<00:00,  1.21it/s]\n100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loss function: 2.5207030891999604\nFold 4, Best Loss:1.452349308207631\nLoaded pretrained weights for efficientnet-b3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 215/215 [01:21<00:00,  2.65it/s]","output_type":"stream"},{"name":"stdout","text":"Done!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = (predictions[0]+predictions[1]+predictions[2]+predictions[3]+predictions[4])/5","metadata":{"execution":{"iopub.status.busy":"2023-03-12T21:41:37.889305Z","iopub.execute_input":"2023-03-12T21:41:37.889784Z","iopub.status.idle":"2023-03-12T21:41:37.895157Z","shell.execute_reply.started":"2023-03-12T21:41:37.889749Z","shell.execute_reply":"2023-03-12T21:41:37.894124Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"Test['Target'] = preds","metadata":{"execution":{"iopub.status.busy":"2023-03-12T21:41:39.289494Z","iopub.execute_input":"2023-03-12T21:41:39.290416Z","iopub.status.idle":"2023-03-12T21:41:39.296652Z","shell.execute_reply.started":"2023-03-12T21:41:39.290366Z","shell.execute_reply":"2023-03-12T21:41:39.295630Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"Test['Target'] = Test['Target'].apply(lambda x: int(x))\nTest","metadata":{"execution":{"iopub.status.busy":"2023-03-12T21:41:40.567213Z","iopub.execute_input":"2023-03-12T21:41:40.567790Z","iopub.status.idle":"2023-03-12T21:41:40.583001Z","shell.execute_reply.started":"2023-03-12T21:41:40.567754Z","shell.execute_reply":"2023-03-12T21:41:40.581716Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"               ImageId  Target\n0    Id_ohk78h9ld8.png       0\n1    Id_eeyj2u4j7y.png       0\n2    Id_wsd7vx2ifa.png      14\n3    Id_6vfneamaoh.png      11\n4    Id_9wil3575fv.png      17\n..                 ...     ...\n853  Id_lmvuv1pm3a.png       0\n854  Id_ez9lb2o6b1.png      37\n855  Id_jeou44iven.png       0\n856  Id_341bsipcnk.png       9\n857  Id_2uc3cx6u47.png       0\n\n[858 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Id_ohk78h9ld8.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Id_eeyj2u4j7y.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Id_wsd7vx2ifa.png</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Id_6vfneamaoh.png</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Id_9wil3575fv.png</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>853</th>\n      <td>Id_lmvuv1pm3a.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>854</th>\n      <td>Id_ez9lb2o6b1.png</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>Id_jeou44iven.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>Id_341bsipcnk.png</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>Id_2uc3cx6u47.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>858 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"Test.to_csv('subensemble_intall.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-12T21:41:54.568135Z","iopub.execute_input":"2023-03-12T21:41:54.568625Z","iopub.status.idle":"2023-03-12T21:41:54.577940Z","shell.execute_reply.started":"2023-03-12T21:41:54.568540Z","shell.execute_reply":"2023-03-12T21:41:54.576703Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"Test","metadata":{"execution":{"iopub.status.busy":"2023-03-12T21:37:47.686630Z","iopub.execute_input":"2023-03-12T21:37:47.686987Z","iopub.status.idle":"2023-03-12T21:37:47.699852Z","shell.execute_reply.started":"2023-03-12T21:37:47.686957Z","shell.execute_reply":"2023-03-12T21:37:47.698636Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"               ImageId  Target\n0    Id_ohk78h9ld8.png       0\n1    Id_eeyj2u4j7y.png       0\n2    Id_wsd7vx2ifa.png      14\n3    Id_6vfneamaoh.png      12\n4    Id_9wil3575fv.png      16\n..                 ...     ...\n853  Id_lmvuv1pm3a.png       0\n854  Id_ez9lb2o6b1.png      37\n855  Id_jeou44iven.png       0\n856  Id_341bsipcnk.png      10\n857  Id_2uc3cx6u47.png       0\n\n[858 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Id_ohk78h9ld8.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Id_eeyj2u4j7y.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Id_wsd7vx2ifa.png</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Id_6vfneamaoh.png</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Id_9wil3575fv.png</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>853</th>\n      <td>Id_lmvuv1pm3a.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>854</th>\n      <td>Id_ez9lb2o6b1.png</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>855</th>\n      <td>Id_jeou44iven.png</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>856</th>\n      <td>Id_341bsipcnk.png</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>857</th>\n      <td>Id_2uc3cx6u47.png</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>858 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}